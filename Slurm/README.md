This is the directory for **Slurm scripts for running the Bio_ClinicalBERT training, fine-tuning, prediction, and hyperparameter search jobs on the Ingham HPC cluster.**

# Scripts in the directory

## Training and Fine-tuning

`parameter_search.slurm` is the **main Slurm job array script for pretraining Bio_ClinicalBERT** on the Ingham GPU cluster.  
It runs 810 jobs in parallel, sweeping combinations of optimizers, learning rates, dropout rates, layers to unfreeze, and seeds.

Each job runs `train.py` with arguments pulled from `parameter_search.csv`, and saves results in `Outputs/models/bcbert_runs/`.  


`finetune_models.slurm` is the **fine-tuning Slurm array script**, designed for running secondary fine-tuning experiments (e.g., from MIMIC to Ingham).  
It reads parameters from `finetune_parameters.csv`, loads pretrained weights, fine-tunes the model on new data, and saves checkpoints in `Outputs/models/cpu_finetune/`.



## Prediction and Evaluation

`predict.slurm` is the **prediction Slurm script** for running `predict.py` on fine-tuned models.  
It loads `model_finetune.pt` and evaluates predictions on a held-out test set.

Outputs include:

- Per-sample predictions
- Evaluation metrics (accuracy, F1, confusion matrix)
- Results saved into the same `bcbert_runs` or `cpu_finetune` directory structure



## Hyperparameter Configuration

`parameter_search.csv` and `finetune_parameters.csv` contain **all hyperparameter search combinations**.  
These include:

- Optimizer (AdamW, Adam, SGD)  
- Learning rate (0.0001, 0.0005, 0.005)  
- Dropout (0.15, 0.2, 0.25)  
- Layer unfreeze (0, 1, 2)  
- Seed (multiple seeds for reproducibility)

These CSVs are generated by `configCreator.py` or manually edited for sweeps.







